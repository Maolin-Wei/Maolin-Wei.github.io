<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jimuyang Zhang</title>

  <meta name="author" content="Jimuyang Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="6qogr8z7RZgR_SCX2GSXZNaoOxokGl59tYYreLe6uEg" />
  <meta property="og:image" content="images_jim/jimuyang_circle.png" />
  <meta name="og:title" content="Jimuyang Zhang" />
  <meta name="title" content="Jimuyang Zhang" />
  <link rel=" stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>


<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Jimuyang Zhang</name>
                  <p>I am a fifth-year PhD student at Boston University, advised by Prof. <a
                      href="https://eshed1.github.io/">Eshed Ohn-Bar</a>.
                    
                  <p>
                    Prior to BU, I worked with Dr. <a href="https://www.donghuang-research.com/">Dong Huang</a> as a research assistant at the <a href="http://humansensing.cs.cmu.edu/">Human
                    Sensing Lab</a>. I got my master's degree (2016-2018) at the Robotics Institute of Carnegie Mellon University, where I worked with Prof. <a href="http://www.cs.cmu.edu/~kkitani/">Kris
                      Kitani</a>, on pedestrian detection on low-profile robot.


                  <p style="text-align:center">
                    <a href="mailto:zhangjimuyang@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="https://github.com/Jimuyangz/">Github</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=9spN7eUAAAAJ&hl=en">Google Scholar</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images_jim/jimuyang_circle.png"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images_jim/jimuyang_circle.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>




          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>My research interests lie in computer vision, robotics and machine learning with their applications in autonomous and assistive systems.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>





              <tr onmouseout="eccv_2024_stop()" onmouseover="eccv_2024_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='eccv_2024_video'><video width=100% muted autoplay loop>
                        <source src="images_jim/Nemo_vid.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='eccv_2024_still'><img src='images_jim/Nemo_pic.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function eccv_2024_start() {
                      document.getElementById('eccv_2024_video').style.display = 'inline';
                      document.getElementById('eccv_2024_still').style.display = 'none';
                    }

                    function eccv_2024_stop() {
                      document.getElementById('eccv_2024_video').style.display = 'none';
                      document.getElementById('eccv_2024_still').style.display = 'inline';
                    }
                    eccv_2024_stop()
                  </script>
                </td>



                <td
                  style="padding-left:20px;padding-right: 20px; padding-top:10px;padding-bottom: 10px;width:75%;vertical-align:middle">
                  <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02571.pdf">
                    <papertitle>Neural Volumetric World Models for Autonomous Driving
                      </papertitle>
                  </a>
                  <br>
                  <strong>Jimuyang Zhang*</strong>, <a href="https://tzmhuang.github.io/">Zanming Huang*,
                   <a href="https://scholar.google.com/citations?user=p9zVBV4AAAAJ&hl=en&oi=ao">Eshed Ohn-Bar</a>
                  <br>
                  <em>European Conference on Computer Vision (ECCV)</em>, 2024
                  <br>
                  <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02571.pdf">paper</a> &nbsp/&nbsp
                  <a href="https://eccv.ecva.net/virtual/2024/poster/250">webpage</a> &nbsp/&nbsp
                  <a href="https://youtu.be/HYFvPfyaZfM">video</a>
                  <p></p>
                  <p>We introduce NeMo, a neural volumetric world modeling approach that can be trained in a self-supervised manner for image reconstruction and 
                    occupancy prediction tasks, benefiting scalable training and deployment paradigms such as imitation learning. We demonstrate how the higher-fidelity 
                    modeling of 3D volumetric representations benefits vision-based motion planning. We propose a motion flow module to model complex dynamic scenes, and
                    introduce a temporal attention module to effectively integrate predicted future volumetric features for the planning task.
                  </p>
                </td>
              </tr>
              <!-- END PAPER NeMo -->


              
               <tr onmouseout="cvpr_2024_stop()" onmouseover="cvpr_2024_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='cvpr_2024_video'><video width=100% muted autoplay loop>
                        <source src="images_jim/FeD_short.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='cvpr_2024_still'><img src='images_jim/fed.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function cvpr_2024_start() {
                      document.getElementById('cvpr_2024_video').style.display = 'inline';
                      document.getElementById('cvpr_2024_still').style.display = 'none';
                    }

                    function cvpr_2024_stop() {
                      document.getElementById('cvpr_2024_video').style.display = 'none';
                      document.getElementById('cvpr_2024_still').style.display = 'inline';
                    }
                    cvpr_2024_stop()
                  </script>
                </td>



                <td
                  style="padding-left:20px;padding-right: 20px; padding-top:10px;padding-bottom: 10px;width:75%;vertical-align:middle">
                  <a href="https://jimuyangz.github.io/papers/FeD_v1.pdf">
                    <papertitle>Feedback-Guided Autonomous Driving
                      </papertitle>
                  </a>
                  <br>
                  <strong>Jimuyang Zhang</strong>, <a href="https://tzmhuang.github.io/">Zanming Huang, <a href="https://arijitray1993.github.io/">Arijit Ray,
                   <a href="https://scholar.google.com/citations?user=p9zVBV4AAAAJ&hl=en&oi=ao">Eshed Ohn-Bar</a>
                  <br>
                  <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024
                  <br>
                  <strong><FONT COLOR="#ff0000">Highlight presentation (~top 2.8% of submitted papers)</FONT></strong>
                  <br>
                  <a href="https://jimuyangz.github.io/papers/FeD_v1.pdf">paper</a> &nbsp/&nbsp
                  <a href="https://fedaltothemetal.github.io/">webpage</a> &nbsp/&nbsp
                  <a href="https://www.youtube.com/watch?v=0f6NQ7sn6hA">video</a>
                  <p></p>
                  <p>We introduce FeD, a highly efficient MLLM-based sensorimotor driving model, enabled by three key improvements: 1) language-based feedback 
                    refining trained using auto-generated feedback data. 2) training the model via distillation from a privileged agent with Bird's Eye View (BEV) 
                    of the scene, allowing our model to robustly use just RGB data at test time. 3) predicting driving waypoints in a masked-token fashion from the 
                    waypoint tokens' internal representations, i.e., not relying on the slow sequentially generative process.
                  </p>
                </td>
              </tr>
              <!-- END PAPER FeD -->




              

              <tr onmouseout="iccv_2023_stop()" onmouseover="iccv_2023_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='iccv_2023_video'><video width=100% muted autoplay loop>
                        <source src="images_jim/xvo_seq.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='iccv_2023_still'><img src='images_jim/xvo.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function iccv_2023_start() {
                      document.getElementById('iccv_2023_video').style.display = 'inline';
                      document.getElementById('iccv_2023_still').style.display = 'none';
                    }

                    function iccv_2023_stop() {
                      document.getElementById('iccv_2023_video').style.display = 'none';
                      document.getElementById('iccv_2023_still').style.display = 'inline';
                    }
                    iccv_2023_stop()
                  </script>
                </td>



                <td
                  style="padding-left:20px;padding-right: 20px; padding-top:10px;padding-bottom: 10px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2309.16772.pdf">
                    <papertitle>XVO: Generalized Visual Odometry via Cross-Modal Self-Training
                      </papertitle>
                  </a>
                  <br>
                  Lei Lai*, Zhongkai Shangguan*, <strong>Jimuyang Zhang</strong>,
                   <a href="https://eshed1.github.io/">Eshed Ohn-Bar</a>
                  <br>
                  <em>International Conference on Computer Vision (ICCV)</em>, 2023
                  <br>
                  <a href="https://arxiv.org/pdf/2309.16772.pdf">paper</a> &nbsp/&nbsp
                  <a href="https://genxvo.github.io/">webpage</a>
                  <p></p>
                  <p>We propose XVO, a semi-supervised learning method for training generalized monocular Visual Odometry (VO) models with 
                    robust off-the-shelf operation across diverse datasets and settings. We empirically demonstrate the benefits of semi-supervised 
                    training for learning a general-purpose direct VO regression network. Moreover, we demonstrate multi-modal supervision, including 
                    segmentation, flow, depth, and audio auxiliary prediction tasks, to facilitate generalized representations for the VO task.
                  </p>
                </td>
              </tr>
              <!-- END PAPER XVO -->


              
              
              <tr onmouseout="cvpr_2023_stop()" onmouseover="cvpr_2023_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='cvpr_2023_video'><video width=100% muted autoplay loop>
                        <source src="images_jim/CaT.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='cvpr_2023_still'><img src='images_jim/CaT.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function cvpr_2023_start() {
                      document.getElementById('cvpr_2023_video').style.display = 'inline';
                      document.getElementById('cvpr_2023_still').style.display = 'none';
                    }

                    function cvpr_2023_stop() {
                      document.getElementById('cvpr_2023_video').style.display = 'none';
                      document.getElementById('cvpr_2023_still').style.display = 'inline';
                    }
                    cvpr_2023_stop()
                  </script>
                </td>



                <td
                  style="padding-left:20px;padding-right: 20px; padding-top:10px;padding-bottom: 10px;width:75%;vertical-align:middle">
                  <a href="https://catdrive.github.io/resources/CaT.pdf">
                    <papertitle>Coaching a Teachable Student
                      </papertitle>
                  </a>
                  <br>
                  <strong>Jimuyang Zhang</strong>, <a href="https://tzmhuang.github.io/">Zanming Huang,
                   <a href="https://scholar.google.com/citations?user=p9zVBV4AAAAJ&hl=en&oi=ao">Eshed Ohn-Bar</a>
                  <br>
                  <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023
                  <br>
                  <strong><FONT COLOR="#ff0000">Highlight presentation (~top 2.5% of submitted papers)</FONT></strong>
                  <br>
                  <a href="https://catdrive.github.io/resources/CaT.pdf">paper</a> &nbsp/&nbsp
                  <a href="https://catdrive.github.io/">webpage</a>
                  <p></p>
                  <p>We propose a novel knowledge distillation framework for effectively teaching a sensorimotor student agent to drive from the 
                    supervision of a privileged teacher agent. Our proposed sensorimotor agent results in a robust image-based behavior cloning 
                    agent in CARLA, improving over current models by over 20.6% in driving score without requiring LiDAR, historical observations, 
                    ensemble of models, on-policy data aggregation or reinforcement learning.
                  </p>
                </td>
              </tr>
              <!-- END PAPER CaT -->
              
              
              
              
              
              
              
              
              
              <tr onmouseout="eccv_2022_stop()" onmouseover="eccv_2022_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='eccv_2022_video'><video width=100% muted autoplay loop>
                        <source src="images_jim/assiter.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='eccv_2022_still'><img src='images_jim/simulation.PNG' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function eccv_2022_start() {
                      document.getElementById('eccv_2022_video').style.display = 'inline';
                      document.getElementById('eccv_2022_still').style.display = 'none';
                    }

                    function eccv_2022_stop() {
                      document.getElementById('eccv_2022_video').style.display = 'none';
                      document.getElementById('eccv_2022_still').style.display = 'inline';
                    }
                    eccv_2022_stop()
                  </script>
                </td>



                <td
                  style="padding-left:20px;padding-right: 20px; padding-top:10px;padding-bottom: 10px;width:75%;vertical-align:middle">
                  <a href="https://eshed1.github.io/papers/assister_eccv2022.pdf">
                    <papertitle>ASSISTER: Assistive Navigation via Conditional Instruction Generation
                      </papertitle>
                  </a>
                  <br>
                  Zanming Huang*, Zhongkai Shangguan*, <strong>Jimuyang Zhang</strong>, Gilad Bar, <a href="https://www.linkedin.com/in/mattcboyd/">Matthew Boyd</a>,
                   <a href="https://scholar.google.com/citations?user=p9zVBV4AAAAJ&hl=en&oi=ao">Eshed Ohn-Bar</a>
                  <br>
                  <em>European Conference on Computer Vision (ECCV)</em>, 2022
                  <br>
                  <a href="https://eshed1.github.io/papers/assister_eccv2022.pdf">paper</a> &nbsp/&nbsp
                  <a href="https://github.com/h2xlab/ASSISTER">data and code</a>
                  <p></p>
                  <p>We introduce a novel vision-and-language navigation (VLN) task of learning to provide real-time guidance to a blind follower situated 
                    in complex dynamic navigation scenarios. We collect a multi-modal real-world benchmark with in-situ Orientation and Mobility (O&M) 
                    instructional guidance. We leverage the real-world study to inform the design of a larger-scale simulation benchmark. In the end, we present ASSISTER, an imitation-learned agent that can 
                    embody such effective guidance.
                  </p>
                </td>
              </tr>
              <!-- END PAPER Assister -->
              
              
              


              <tr onmouseout="cvpr_2022_stop()" onmouseover="cvpr_2022_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='cvpr_2022_video'><video width=100% muted autoplay loop>
                        <source src="images_jim/selfd_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='cvpr_2022_still'><img src='images_jim/selfd_f1.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function cvpr_2022_start() {
                      document.getElementById('cvpr_2022_video').style.display = 'inline';
                      document.getElementById('cvpr_2022_still').style.display = 'none';
                    }

                    function cvpr_2022_stop() {
                      document.getElementById('cvpr_2022_video').style.display = 'none';
                      document.getElementById('cvpr_2022_still').style.display = 'inline';
                    }
                    cvpr_2022_stop()
                  </script>
                </td>



                <td
                  style="padding-left:20px;padding-right: 20px; padding-top:10px;padding-bottom: 10px;width:75%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_SelfD_Self-Learning_Large-Scale_Driving_Policies_From_the_Web_CVPR_2022_paper.pdf">
                    <papertitle>SelfD: Self-Learning Large-Scale Driving Policies From the Web
                      </papertitle>
                  </a>
                  <br>
                  <strong>Jimuyang Zhang</strong>,
                  <a href="https://scholar.google.com/citations?user=otVAkGkAAAAJ&hl=en">Ruizhao Zhu</a>,
                  <a href="https://scholar.google.com/citations?user=p9zVBV4AAAAJ&hl=en&oi=ao">Eshed Ohn-Bar</a>
                  <br>
                  <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
                  <br>
                  <a href="https://arxiv.org/pdf/2204.10320.pdf">arxiv</a> &nbsp/&nbsp
                  <a href="https://www.youtube.com/watch?v=nWhdxx_9o58">video</a>
                  <p></p>
                  <p>We introduce SelfD, a framework for learning scalable driving by utilizing large amounts of online monocular images. Our key idea is to leverage iterative semi-supervised training when learning imitative agents from unlabeled data. We employ a large dataset of publicly available YouTube videos to train SelfD and comprehensively analyze its generalization benefits across challenging navigation scenarios. SelfD demonstrates consistent improvements (by up to 24%) in driving performance evaluation on nuScenes, Argoverse, Waymo, and CARLA.
                  </p>
                </td>
              </tr>
              <!-- END PAPER Selfd -->



            <tr onmouseout="iccv_2021_stop()" onmouseover="iccv_2021_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='iccv_2021_video'><video width=100% muted autoplay loop>
                        <source src="images_jim/xworld_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='iccv_2021_still'><img src='images_jim/xworld.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function iccv_2021_start() {
                      document.getElementById('iccv_2021_video').style.display = 'inline';
                      document.getElementById('iccv_2021_still').style.display = 'none';
                    }

                    function iccv_2021_stop() {
                      document.getElementById('iccv_2021_video').style.display = 'none';
                      document.getElementById('iccv_2021_still').style.display = 'inline';
                    }
                    iccv_2021_stop()
                  </script>
                </td>



                <td
                  style="padding-left:20px;padding-right: 20px; padding-top:10px;padding-bottom: 10px;width:75%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_X-World_Accessibility_Vision_and_Autonomy_Meet_ICCV_2021_paper.pdf">
                    <papertitle>X-World: Accessibility, Vision, and Autonomy Meet</papertitle>
                  </a>
                  <br>
                  <strong>Jimuyang Zhang*</strong>,
                  <a href="https://www.linkedin.com/in/minglan-zheng-611b59174/">Minglan Zheng*</a>,
                  <a href="https://www.linkedin.com/in/mattcboyd/">Matthew Boyd</a>,
                  <a href="https://scholar.google.com/citations?user=p9zVBV4AAAAJ&hl=en&oi=ao">Eshed Ohn-Bar</a>
                  <br>
                  <em>International Conference on Computer Vision (ICCV)</em>, 2021
                  <br>
                  <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_X-World_Accessibility_Vision_and_Autonomy_Meet_ICCV_2021_paper.pdf">arxiv</a> &nbsp/&nbsp
                  <a href="https://www.youtube.com/watch?v=z_YwWIZWg58">video</a> &nbsp/&nbsp
                  <a href="https://www.youtube.com/watch?v=9EiX4BxC2b8">talk</a> &nbsp/&nbsp
                  <a href="https://accessibility-cv.github.io/">workshop</a> &nbsp/&nbsp
                  <a href="https://eval.ai/web/challenges/challenge-page/1690/overview">challenge</a>
                  <p></p>
                  <p>We introduce X-World, an accessibility-centered development environment for vision-based autonomous systems, which enables spawning dynamic agents with various mobility aids. The simulation supports generation of ample amounts of finely annotated, multi-modal data in a safe, cheap, and privacy-preserving manner. We highlight novel difficulties introduced by our benchmark and tasks, as well as opportunities for future developments. We further validate and extend our analysis by introducing a complementary real-world evaluation benchmark.
                  </p>
                </td>
              </tr>
              <!-- END PAPER xworld -->






              <tr onmouseout="cvpr_2021_stop()" onmouseover="cvpr_2021_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='cvpr_2021_video'><video width=100% muted autoplay loop>
                        <source src="images_jim/lbw_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='cvpr_2021_still'><img src='images_jim/lbw.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function cvpr_2021_start() {
                      document.getElementById('cvpr_2021_video').style.display = 'inline';
                      document.getElementById('cvpr_2021_still').style.display = 'none';
                    }

                    function cvpr_2021_stop() {
                      document.getElementById('cvpr_2021_video').style.display = 'none';
                      document.getElementById('cvpr_2021_still').style.display = 'inline';
                    }
                    cvpr_2021_stop()
                  </script>
                </td>



                <td
                  style="padding-left:20px;padding-right: 20px; padding-top:10px;padding-bottom: 10px;width:75%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_by_Watching_CVPR_2021_paper.pdf">
                    <papertitle>Learning by Watching</papertitle>
                  </a>
                  <br>
                  <strong>Jimuyang Zhang</strong>,
                  <a href="https://scholar.google.com/citations?user=p9zVBV4AAAAJ&hl=en&oi=ao">Eshed Ohn-Bar</a>
                  <br>
                  <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021
                  <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_by_Watching_CVPR_2021_paper.pdf">arxiv</a> &nbsp/&nbsp
                  <a href="https://www.youtube.com/watch?v=ApyIcoTDyc8">video</a>
                  <p></p>
                  <p>We propose the Learning by Watching (LbW) framework which enables learning a driving policy without requiring full knowledge of neither the state nor expert actions. To increase its data, i.e., with new perspectives and maneuvers, LbW makes use of the demonstrations of other vehicles in a given scene by (1) transforming the egovehicle‚Äôs observations to their points of view, and (2) inferring their expert actions. Our LbW agent learns more robust driving policies while enabling data-efficient learning, including quick adaptation of the policy to rare and novel scenarios.
                  </p>
                </td>
              </tr>
              <!-- END PAPER lbw -->
              
              
              
              
              <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div id='tracking'><img src='images_jim/tracking.png' width=100%></div>
                  </div>
                </td>



                <td
                  style="padding-left:20px;padding-right: 20px; padding-top:10px;padding-bottom: 10px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2001.11180.pdf">
                    <papertitle>Multiple Object Tracking by Flowing and Fusing</papertitle>
                  </a>
                  <br>
                  <strong>Jimuyang Zhang</strong>,
                  <a href="https://scholar.google.com/citations?user=2Drvv44AAAAJ&hl=zh-CN">Sanping Zhou</a>,
                  Xin Chang, Fangbin Wan,
                  <a href="https://scholar.google.com/citations?user=Dk7JgNcAAAAJ&hl=zh-CN">Jinjun Wang</a>,
                  <a href="https://scholar.google.com.hk/citations?user=vwOQ-UIAAAAJ&hl=zh-CN">Yang Wu</a>,
                  <a href="https://scholar.google.com/citations?user=VeXMKBoAAAAJ&hl=zh-CN">Dong Huang</a>
                  <br>
                  <em>arXiv</em>, 2020
                  <br>
                  <a href="https://arxiv.org/pdf/2001.11180.pdf">arxiv</a>
                  <p></p>
                  <p>We design an end-to-end DNN tracking approach, Flow-Fuse-Tracker (FFT), consisting of two efficient techniques: target flowing and target fusing. In target flowing, a FlowTracker DNN module learns the indefinite number of target-wise motions jointly from pixel-level optical flows. In target fusing, a FuseTracker DNN module refines and fuses targets proposed by FlowTracker and frame-wise object detection, instead of trusting either of the two inaccurate sources of target proposal. 
                  </p>
                </td>
              </tr>
              <!-- END PAPER tracking -->











            </tbody>
          </table>








          <!-- Teaching -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Teaching</heading>

                  <p>
                    <a href="https://www.bu.edu/academics/eng/courses/eng-ec-500/">ENG EC 500 - Robot Learning and Vision for Navigation - Spring 2023</a>
                    <br>
                    Teaching Assistant
                  </p>
                  
                  <p>
                    <a href="https://www.bu.edu/academics/eng/courses/eng-ec-444/">ENG EC 444 - Smart and Connected Systems - Fall 2022</a>
                    <br>
                    Teaching Assistant
                  </p>

                  <p>
                    <a href="https://www.bu.edu/academics/eng/courses/eng-ec-503/">ENG EC 503 - Introduction to Learning from Data - Fall 2021</a>
                    <br>
                    Teaching Assistant
                  </p>

                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>


            </tbody>
          </table>



          <!-- Service -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Service</heading>

                  <p>
                    <a href="https://accessibility-cv.github.io/">CVPR2022 AVA Accessibility Vision and Autonomy Challenge</a>
                    <br>
                    Challenge Organizer
                  </p>

                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>


            </tbody>
          </table>










          <tables
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    This website was forked from <a href="https://github.com/jonbarron/jonbarron_website">source
                      code</a>

                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>




